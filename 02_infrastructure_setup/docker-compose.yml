version: '3.8' # Gunakan versi yang didukung oleh Docker Engine Anda

networks:
  bigdata_network:
    driver: bridge

volumes:
  # Volume untuk persistensi data HDFS
  namenode_data:
  datanode1_data:
  datanode2_data: # Jika Anda menggunakan lebih dari 1 datanode

  # Volume untuk metadata Hive (jika menggunakan Derby lokal atau PostgreSQL/MySQL container)
  hive_metastore_data:
  postgres_data: # Jika menggunakan PostgreSQL untuk Hive Metastore & Airflow

  # Volume untuk skrip, notebooks, dan data proyek
  project_scripts:
  project_notebooks:
  project_data_raw: # Untuk data mentah yang akan di-ingest
  airflow_dags:
  airflow_logs:
  airflow_plugins:

services:
  #-------------------------------------
  # Layanan Hadoop (HDFS & YARN)
  #-------------------------------------
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8 # Ganti dengan image/build Anda
    container_name: namenode
    hostname: namenode
    restart: always
    ports:
      - "9870:9870" # NameNode Web UI
      - "9000:9000" # HDFS IPC
    volumes:
      - namenode_data:/hadoop/dfs/name
      - ./02_infrastructure_setup/cluster_init_scripts/:/opt/cluster_init_scripts/ # Untuk init_hdfs_dirs.sh
      - ./01_data_acquisition/ingestion_scripts/:/opt/ingestion_scripts/ # Untuk skrip ingesti
      - ./01_data_acquisition/raw_datasets/:/opt/raw_datasets/ # Untuk akses data mentah dari skrip ingesti
    environment:
      - CLUSTER_NAME=medan_traffic_cluster
      - HDFS_CONF_dfs_replication=1 # Untuk setup lokal dengan sedikit datanode
    env_file:
      - ./02_infrastructure_setup/docker_configs/hadoop/hadoop.env # File konfigurasi environment Hadoop
    networks:
      - bigdata_network

  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8 # Ganti dengan image/build Anda
    container_name: datanode1
    hostname: datanode1
    restart: always
    volumes:
      - datanode1_data:/hadoop/dfs/data
    environment:
      - SERVICE_PRECONDITION=namenode:9870 # Tunggu namenode siap
    env_file:
      - ./02_infrastructure_setup/docker_configs/hadoop/hadoop.env
    depends_on:
      - namenode
    networks:
      - bigdata_network

  # Tambahkan datanode2 jika perlu, dengan konfigurasi serupa datanode1
  # datanode2:
  #   ...

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8 # Ganti dengan image/build Anda
    container_name: resourcemanager
    hostname: resourcemanager
    restart: always
    ports:
      - "8088:8088" # YARN Web UI
    environment:
      - SERVICE_PRECONDITION=namenode:9000 # Sesuaikan dengan kondisi yang tepat
    env_file:
      - ./02_infrastructure_setup/docker_configs/hadoop/hadoop.env
    depends_on:
      - namenode
      - datanode1 # dan datanode lainnya
    networks:
      - bigdata_network

  nodemanager1:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8 # Ganti dengan image/build Anda
    container_name: nodemanager1
    hostname: nodemanager1
    restart: always
    environment:
      - SERVICE_PRECONDITION=resourcemanager:8088
    env_file:
      - ./02_infrastructure_setup/docker_configs/hadoop/hadoop.env
    depends_on:
      - resourcemanager
    networks:
      - bigdata_network

  #-------------------------------------
  # Layanan Apache Spark
  #-------------------------------------
  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3 # Ganti dengan image/build Anda yang sesuai dengan versi Hadoop
    container_name: spark-master
    hostname: spark-master
    restart: always
    ports:
      - "8080:8080" # Spark Master Web UI
      - "7077:7077" # Spark Master RPC
    volumes:
      - ./03_data_processing_pipeline/etl_spark_scripts/:/opt/spark_scripts/etl/
      - ./05_model_development/training_scripts/:/opt/spark_scripts/modeling/
      - ./05_model_development/feature_engineering_scripts/:/opt/spark_scripts/feature_engineering/
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - SPARK_PUBLIC_DNS=localhost # Atau IP publik jika diakses dari luar Docker host
    networks:
      - bigdata_network

  spark-worker-1:
    image: bde2020/spark-worker:3.3.0-hadoop3.3 # Ganti dengan image/build Anda
    container_name: spark-worker-1
    hostname: spark-worker-1
    restart: always
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1 # Sesuaikan dengan resource Anda
      - SPARK_WORKER_MEMORY=2g # Sesuaikan
    depends_on:
      - spark-master
    networks:
      - bigdata_network

  # Tambahkan spark-worker-2 jika perlu
  # spark-worker-2:
  #   ...

  #-------------------------------------
  # Layanan Apache Hive
  #-------------------------------------
  # (Opsional) PostgreSQL untuk Hive Metastore dan Airflow Backend
  postgres-db:
    image: postgres:13-alpine
    container_name: postgres-db
    hostname: postgres-db
    restart: always
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=hiveuser
      - POSTGRES_PASSWORD=hivepassword
      - POSTGRES_DB=hivedb
      # Variabel untuk Airflow (jika menggunakan DB yang sama)
      - POSTGRES_USER_AIRFLOW=airflow
      - POSTGRES_PASSWORD_AIRFLOW=airflow
      - POSTGRES_DB_AIRFLOW=airflowdb
    ports:
      - "5432:5432"
    networks:
      - bigdata_network

  hive-metastore:
    image: bde2020/hive:3.1.2-postgresql-metastore # Image ini sudah dikonfigurasi untuk PostgreSQL
    # atau build: ./02_infrastructure_setup/docker_configs/hive/ # Jika Anda punya Dockerfile kustom
    container_name: hive-metastore
    hostname: hive-metastore
    restart: always
    env_file:
      - ./02_infrastructure_setup/docker_configs/hive/hive.env # File ini berisi konfigurasi koneksi ke DB
    environment:
      - SERVICE_PRECONDITION=namenode:9870 datanode1:9864 postgres-db:5432 # Tunggu layanan lain siap
    depends_on:
      - namenode
      - datanode1
      - postgres-db # Jika menggunakan PostgreSQL
    networks:
      - bigdata_network

  hive-server:
    image: bde2020/hive:3.1.2-postgresql-metastore # Atau build Anda
    container_name: hive-server
    hostname: hive-server
    restart: always
    env_file:
      - ./02_infrastructure_setup/docker_configs/hive/hive.env
    environment:
      - SERVICE_PRECONDITION=hive-metastore:9083 # Tunggu metastore siap
    ports:
      - "10000:10000" # HiveServer2 Thrift port
      - "10002:10002" # Hive Web UI (jika ada di image)
    depends_on:
      - hive-metastore
    networks:
      - bigdata_network

  #-------------------------------------
  # (Opsional) Layanan Apache Airflow
  #-------------------------------------
  # airflow-scheduler:
  #   image: apache/airflow:2.8.0 # Ganti dengan versi dan build Anda
  #   container_name: airflow-scheduler
  #   hostname: airflow-scheduler
  #   restart: always
  #   depends_on:
  #     - postgres-db # Jika menggunakan PostgreSQL untuk backend Airflow
  #   volumes:
  #     - ./03_data_processing_pipeline/airflow_dags/:/opt/airflow/dags
  #     - airflow_logs:/opt/airflow/logs
  #     - airflow_plugins:/opt/airflow/plugins
  #     - ./02_infrastructure_setup/docker_configs/airflow/airflow.cfg:/opt/airflow/airflow.cfg # Konfigurasi Airflow
  #   environment:
  #     - AIRFLOW__CORE__EXECUTOR=LocalExecutor # Atau CeleryExecutor, dll.
  #     - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres-db:5432/airflowdb
  #     # Variabel lain yang dibutuhkan Airflow
  #   command: scheduler
  #   networks:
  #     - bigdata_network

  # airflow-webserver:
  #   image: apache/airflow:2.8.0
  #   container_name: airflow-webserver
  #   hostname: airflow-webserver
  #   restart: always
  #   depends_on:
  #     - airflow-scheduler
  #   ports:
  #     - "8081:8080" # Airflow Web UI (gunakan port berbeda jika 8080 sudah dipakai Spark)
  #   volumes:
  #     - ./03_data_processing_pipeline/airflow_dags/:/opt/airflow/dags
  #     - airflow_logs:/opt/airflow/logs
  #     - airflow_plugins:/opt/airflow/plugins
  #     - ./02_infrastructure_setup/docker_configs/airflow/airflow.cfg:/opt/airflow/airflow.cfg
  #   environment:
  #     - AIRFLOW__CORE__EXECUTOR=LocalExecutor
  #     - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres-db:5432/airflowdb
  #   command: webserver
  #   networks:
  #     - bigdata_network

  #-------------------------------------
  # Layanan Apache Superset (Visualisasi)
  #-------------------------------------
  superset:
    image: apache/superset:latest # Atau versi spesifik
    container_name: superset
    hostname: superset
    restart: always
    ports:
      - "8089:8088" # Superset Web UI (gunakan port berbeda jika 8088 sudah dipakai YARN)
    volumes:
      # - ./02_infrastructure_setup/docker_configs/superset/superset_config.py:/app/pythonpath/superset_config.py # Untuk konfigurasi kustom
      - superset_data:/var/lib/superset # Untuk persistensi metadata Superset (jika menggunakan SQLite internal)
    environment:
      # - SUPERSET_CONFIG_PATH=/app/pythonpath/superset_config.py
      # Variabel lain jika Superset dikonfigurasi dengan DB eksternal
      - ADMIN_USERNAME=admin
      - ADMIN_EMAIL=admin@superset.com
      - ADMIN_PASSWORD=admin
      - SUPERSET_SECRET_KEY=your_strong_secret_key_here # GANTI INI!
      - FLASK_APP=superset
    # command: >
    #   bash -c " \
    #   superset fab create-admin && \
    #   superset db upgrade && \
    #   superset init && \
    #   superset load_examples && \
    #   superset run -h 0.0.0.0 -p 8088 --with-threads --reload --debugger \
    #   "
    # (Command di atas mungkin perlu disesuaikan berdasarkan image Superset yang digunakan)
    depends_on:
      - postgres-db # Jika Superset menggunakan PostgreSQL
      - hive-server # Agar bisa dikoneksikan ke Hive
    networks:
      - bigdata_network

  #-------------------------------------
  # Layanan Jupyter Notebook (Opsional, untuk EDA & Development)
  #-------------------------------------
  # jupyterlab:
  #   image: jupyter/pyspark-notebook:latest # Image ini sudah ada Spark
  #   container_name: jupyterlab
  #   hostname: jupyterlab
  #   ports:
  #     - "8888:8888" # JupyterLab Web UI
  #   volumes:
  #     - ./04_exploratory_data_analysis/:/home/jovyan/work/04_exploratory_data_analysis
  #     - ./05_model_development/:/home/jovyan/work/05_model_development
  #     - ./01_data_acquisition/raw_datasets/:/home/jovyan/work/data/raw # Untuk akses data mentah dari notebook
  #   environment:
  #     - JUPYTER_ENABLE_LAB=yes
  #     - GRANT_SUDO=yes # Hati-hati dengan ini di produksi
  #     # Konfigurasi untuk konek ke Spark Master Anda jika image tidak otomatis terkoneksi
  #     # - SPARK_MASTER=spark://spark-master:7077
  #   user: "root" # Atau user jovyan dengan sudo
  #   networks:
  #     - bigdata_network